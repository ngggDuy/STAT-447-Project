---
title: "Variational Bayesian Inference Methods in the Prediction of Share Closing Prices"
author: "Duy Nguyen & Edward Bai"
date: "2024-03-16"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tseries)
library(rstan)
library(tidyr)
library(dplyr)
```

```{r data-prep, echo=FALSE}
#####===== Data Pre-processing =====#####
# Unemployment rate dataset
unemployment.data <- read.csv("./data/unemployment.csv")

unemployment <- unemployment.data %>%
  select(TIME_PERIOD, OBS_VALUE) %>%
  rename(UNEMPLOYMENT_RATE = OBS_VALUE)

unemployment <- unemployment[1:(length(unemployment$TIME_PERIOD) - 2),]

# Consumer Price Index (Inflation) dataset
cpi.data <- read.csv("./data/inflation.csv")

inflation <- cpi.data %>%
  select(TIME_PERIOD, OBS_VALUE) %>%
  rename(CPI = OBS_VALUE)

inflation <- inflation[1:(length(inflation$TIME_PERIOD) - 1),]

# Industrial Production Index dataset
industrial.data <- read.csv("./data/industry.csv")

industrial.data.cleaned <- industrial.data %>% 
  select(TIME_PERIOD, Economic.activity, OBS_VALUE) %>%
  rename(IPIndex = OBS_VALUE)

industrial <- industrial.data.cleaned %>%
  group_by(TIME_PERIOD, Economic.activity) %>%
  pivot_wider(names_from = Economic.activity, values_from = IPIndex) 

# Interest Rate dataset
interest.data <- read.csv("./data/interest_rates.csv")

interest.data.cleaned <- interest.data %>%
  select(Measure, TIME_PERIOD, OBS_VALUE) %>%
  rename(INTEREST_RATE = OBS_VALUE)

interest <- interest.data.cleaned %>%
  group_by(TIME_PERIOD, Measure) %>%
  pivot_wider(names_from = Measure, values_from = INTEREST_RATE)

interest <- interest %>%
  select(-`Immediate interest rates, call money, interbank rate`)

interest <- interest[1:(length(interest$TIME_PERIOD) - 14),]

# TSX Index dataset 
tsx_data <- read.csv("./data/TSX.csv")
tsx_data <- tsx_data[1:(length(tsx_data$Date) - 3), ]

# Combined dataframe construction

# The TSX index dataset is the shortest so we base the combined dataframe's 
# length on it
min_rows <- min(length(unemployment$TIME_PERIOD), 
                length(inflation$TIME_PERIOD), 
                length(industrial$TIME_PERIOD), 
                length(interest$TIME_PERIOD), 
                length(tsx_data$Date))

# Since constituent dataframes have varying starting and end dates,
# we have to trim the data (some done above)

# Calculate the starting row index for each dataframe
start_index_unemployment <- length(unemployment$TIME_PERIOD) - min_rows + 1
start_index_inflation <- length(inflation$TIME_PERIOD) - min_rows + 1
start_index_industrial <- length(industrial$TIME_PERIOD) - min_rows + 1
start_index_interest <- length(interest$TIME_PERIOD) - min_rows + 1

# Select rows from the calculated starting index to the end
unemployment <- unemployment[start_index_unemployment:length(unemployment$TIME_PERIOD), ]
inflation    <- inflation[start_index_inflation:length(inflation$TIME_PERIOD), ]
industrial   <- industrial[start_index_industrial:length(industrial$TIME_PERIOD), ]
interest     <- interest[start_index_interest:length(interest$TIME_PERIOD), ]

#####===== Data Transformations & Final Cleaning =====#####
# De-trending by fitting simple linear models to predictors and response 
# for better model interpretability
unemployment$TIME_INDEX <-  as.numeric(as.factor(unemployment$TIME_PERIOD))
fit_unemployment <- lm(unemployment$UNEMPLOYMENT_RATE ~ unemployment$TIME_INDEX, data=unemployment)

inflation$TIME_INDEX <-  as.numeric(as.factor(inflation$TIME_PERIOD))
fit_inflation <- lm(inflation$CPI ~ inflation$TIME_INDEX, data=inflation)

industrial$TIME_INDEX <-  as.numeric(as.factor(industrial$TIME_PERIOD))
fit_industry <- lm(industrial$`Industry (except construction)` ~ industrial$TIME_INDEX, data=industrial)
fit_construction <- lm(industrial$Construction ~ industrial$TIME_INDEX, data=industrial)
fit_manufacturing <- lm(industrial$Manufacturing ~ industrial$TIME_INDEX, data=industrial)

interest$TIME_INDEX <-  as.numeric(as.factor(interest$TIME_PERIOD))
fit_long_rates <- lm(interest$`Long-term interest rates` ~ interest$TIME_INDEX, data=interest)
fit_short_rates <- lm(interest$`Short-term interest rates` ~ interest$TIME_INDEX, data=interest)

# Standardize predictor variables 
standardized_unemployment_residuals <- scale(fit_unemployment$residuals)
unemployment$UNEMPLOYMENT_RATE <- standardized_unemployment_residuals

standardized_inflation_residuals <- scale(fit_inflation$residuals)
inflation$CPI <- standardized_inflation_residuals

standardized_industry_residuals <- scale(fit_industry$residuals)
industrial$`Industry (except construction)` <- standardized_industry_residuals 
standardized_construction_residuals <- scale(fit_construction$residuals)
industrial$Construction <-standardized_construction_residuals
standardized_manufacturing_residuals <- scale(fit_manufacturing$residuals)
industrial$Manufacturing <-standardized_manufacturing_residuals

standardized_short_rates_residuals <- scale(fit_long_rates$residuals)
interest$`Short-term interest rates` <- standardized_short_rates_residuals
standardized_long_rates_residuals <- scale(fit_short_rates$residuals)
interest$`Long-term interest rates` <- standardized_long_rates_residuals

data.combined <- bind_cols(unemployment, inflation, industrial, interest, tsx_data)

# Open, High, Low, Close, Adj.Close, Volume
# Final cleanup

# Predictors
predictors <- data.combined %>% 
  select(UNEMPLOYMENT_RATE, CPI, 
         Construction, `Industry (except construction)`, Manufacturing, 
         `Short-term interest rates`, `Long-term interest rates`)

# Response
response <- data.combined %>% 
  select(Date, Close)

# Log-transform the TSX index
response$Close_log <- log(response$Close)
response$Date <-  as.numeric(as.factor(response$Date))
fit_response <- lm(response$Close_log ~ response$Date, data=response)
response$Close_log_residuals <- fit_response$residuals

URate_and_IRate_predictors <- predictors %>%
  select(UNEMPLOYMENT_RATE, `Long-term interest rates`)

construction_predictors <- predictors %>% 
  select(Construction, `Industry (except construction)`, Manufacturing)

```

## Exploratory Data Analysis

```{r data-explore, echo=FALSE, include=FALSE}
plot(inflation$CPI)
plot(unemployment$UNEMPLOYMENT_RATE)
plot(industrial$Construction)
plot(industrial$`Industry (except construction)`)
plot(industrial$Manufacturing)

plot(interest$`Short-term interest rates`)
plot(interest$`Long-term interest rates`)
```

## Model Description

```{stan output.var = "stock"}
data {
  int<lower=1> N;             // Number of data points
  int<lower=1> K;             // Number of predictors
  matrix[N, K] X;             // Predictor matrix
  vector[N] y;                // Response variable (TSX index)
}

parameters {
  vector[K] beta;             // Coefficients for predictors
  real<lower=0> sigma;        // Standard deviation of the residuals
}

model {
  // Priors
  beta ~ normal(0, 1);        // Standard normal prior for beta coefficients
  sigma ~ cauchy(0, 2.5);     // Weakly informative prior for sigma
  
  // Likelihood
  y ~ normal(X * beta, sigma);
}
```

```{stan output.var = "stockmixture"}
data {
  int<lower=1> N;             // Number of data points
  int<lower=1> K;             // Number of predictors
  matrix[N, K] X;             // Predictor matrix
  vector[N] y;                // Response variable (TSX index)
  int<lower=1> num_components; // Number of components in the mixture
}

parameters {
  vector[K] beta[num_components];   // Coefficients for each component
  real<lower=0> sigma[num_components]; // Standard deviation for each component
  simplex[num_components] mixing_proportions; // Mixing proportions for each component
}

model {
  // Priors
  for (n in 1:num_components) {
    beta[n] ~ normal(0, 1);
    sigma[n] ~ cauchy(0, 2.5);
  }
  
  // Mixture likelihood
  for (i in 1:N) {
    vector[num_components] component_likelihoods;
    for (n in 1:num_components) {
      component_likelihoods[n] = log(mixing_proportions[n]) + 
      normal_lpdf(y[i] | X[i] * beta[n], sigma[n]);
    }
    target += log_sum_exp(component_likelihoods);
  }
}
```

```{r model-fitting, echo=TRUE}
stan_data <- list(N = nrow(predictors),
                  K = ncol(predictors),  
                  X = predictors,
                  y = response$Close_log_residuals)

# Fit stan model with variational baye's using ADVI
fit_sampling <- sampling(stock, data = stan_data,
          iter = 1000)

plot(stan_trace(fit_sampling))

# Prepare data list for stan
stan_data <- list(N = nrow(predictors),
                  K = ncol(predictors),  
                  X = predictors,
                  y = response$Close_log_residuals,
                  num_components = 2)

# Fit stan model with variational baye's using ADVI
fit <- vb(stockmixture, data = stan_data,
          iter = 10000,  # Increase if needed to achieve better convergence
          tol_rel_obj = 0.0002,  # Adjusted tolerance for stopping criterion
          output_samples = 1000,  # Number of posterior samples to draw
          importance_resampling = TRUE,
          pars=c("beta", "sigma"),
          include=TRUE,
          elbo_samples = 250,
          init=0,
          algorithm="meanfield")  # Enable importance resampling

plot(stan_trace(fit))


```

## Introduction

The ability to forecast accurately whilst taking into account known information is important across all fields and industries. In economics, especially when money-making is concerned, the stakes become even higher thereby demanding a certain degree of accuracy with these predictions. Thus, statisticians and data scientists have come up with many ways to formulate prediction models, to help inform business or investment decisions, that are becoming increasingly accurate as contemporary research bolsters existing methods.

Many of these methods consist of models that involve complex relationships between numerous variables, some of which are latent or unobserved meaning that some models will fare better than others. This study seeks to evaluate a number of these models against one another in the context of forecasting stock price movements to ascertain their respective performances. Specifically, we will be attempting to identify whether key macroeconomic indicators can be used to inform, and therefore improve forecasting techniques. With this in mind, we will inherently be working with large datasets, requiring the use of methods appropriate for large datasets, namely Variational Bayesian Inference models.

Why do we want to use Variational Bayes (VB) for this scenario? This is because VB is often more scalable to large datasets compared to sampling-based methods like Markov Chain Monte Carlo (MCMC). For models where the resulting posterior distributions are likely to be complex and intractable due to involving high-dimensional spaces (or non-conjugate priors), VB methods provide a way to approximate these distributions. Lastly, VB’s output provides a measure that enables the comparison and selection of different potential models. (Chérief-Abdellatif & Alquier, 2018) The rest of the paper will detail the dataset under study as well as the employed methodologies and their evaluations.

## Data Preparation & Analysis

Acknowledging the glaring differences between economically developed and currently developing economies, we initially sought data from two different regions to make for a better comparison. However, with this difference also comes the difficulty of data collection and subsequently determining its authenticity. Several nations do not have the agency or capability to reliably report data for various reasons. From this, we narrowed our search to regions that have renowned agencies with established systems of collecting and reporting data. In addition to this, we wanted our model to be based on a globally connected economy that is likely to have significant economic reactions to international factors such as war or recessions. The Organisation for Economic Co-operation and Development (OECD) is a forum consisting of 37 democracies with market-based economies with the purpose of joint development of policy standards to encourage sustainable economic growth. The OECD has an abundant collection of data on various economic indicators that we could use and so we settled on focusing on data for the Canadian economy because it best matches the aforementioned criteria.

Forecasting the movement of any particular stock is rather difficult as this necessitates a model that takes into account the myriad factors that can influence changes in stock price. Attempting to build a model that encompasses all of these potential factors will ultimately not do any good as this risks upsetting the model complexity tradeoff and the model ends up being over-fit or just not meaningful. To mitigate this, we will instead forecast the closing values of the S&P/TSX Composite Index, a benchmark Canadian index representing approximately 70% of the total market capitalization on the Toronto Stock Exchange. The closing price is the 'raw' price which is just the cash value of the last transacted price before the market closes.

Regarding our predictor variables, we have 4 different economic indicators for Canada, as follows.

-   Unemployment rate $U_t$
-   Consumer Price Index $\text{CPI}_t$ (in percentage per annum)
-   Industrial Production Indices $I$
-   $I_{S, t}$ : Total industrial production
-   $I_{M, t}$ : Manufacturing
-   $I_{C, t}$ : Construction
-   Interest Rates $R$ (in percentage per annum)
-   $R_{\text{long}, t}$ : Long term
-   $R_{\text{short}, t}$ : Short term

Briefly speaking, these variables were chosen due to the impact and shocks they tend to have on stock market conditions. For example, high unemployment rates may indicate a faltering economy, which may lower investor confidence and hurt the performance of the stock market. Excessive inflation can reduce buying power, put pressure on company profit margins, and weaken stock values. Industrial Production indices measure real output in the three listed sectors and rising values tend to signal economic growth and, in turn, stock prices.

## Model Structure

We will employ a Bayesian linear regression model where the response variable $\text{TSX}_t$ is regressed on the economic indicators listed above:

$$\text{TSX}_{t} = \beta_0 + \beta_1U_{t} + \beta_2CPI_{t} + \beta_3I_{\text{S}, t} + \beta_4IP_{\text{M}, t} + \beta_5IP_{\text{C}, t} + \beta_6R_{\text{long}, t} + \beta_7R_{\text{short}, t} + \epsilon_t$$

It will be computationally prohibitive to approximate the posterior distributions of the model parameters (i.e. Beta coefficients, sigma) given the nature of our data. The high-dimensional data space will lead to high-dimensional integration (due to the number of variables and their cross-interactions) that are not analytically tractable. Here, Variational Bayes provides a practical solution by approximating these complex posteriors with simpler distributions through a Variational Family which is a family of distributions that approximates the posteriors. We will be using the common choice of the mean-field variational family in which the joint distribution of the model parameters is approximated by a product of independent distributions for each parameter:

$$q(\theta) = q(\beta_0) \times q(\beta_1) \times \cdots \times q(\beta_7) \times q(\sigma^2)$$

Here, $\theta$ represents all the parameters in the model and each Beta and sigma is assumed to have its own distinct distribution q, typically chosen to be in the same family as the prior (normal and inverse-gamma).

In Bayesian linear regression, using a normal prior for the regression coefficients ($\beta$) is a natural choice when the likelihood is also normal (as is the case with the residuals). This combination leads to a normal posterior, which is a conjugate prior for the normal likelihood and this will simplify computations.
